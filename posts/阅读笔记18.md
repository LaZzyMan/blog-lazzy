---
title: Explanatory Model Analysis：Explore, Explain, and Examine Predictive Models.
date: '2021-04-19 10:23:00'
tags: 
- 论文

category: Research
image: /img/ema.png

[原书地址](https://ema.drwhy.ai/)

## Introduction

书中指出，在1942年阿西莫夫在Runaround中制定了机器人的三条准则：
- a robot may not injure a human being.
- a robot must obey the orders given it by human beings.
- a robot must protect its own existence.
而现实中，与物理机器人相比，模型和算法对我们的生活影响更大，并且尽管有潜在危害的例子，但此类模型的应用仍不受管制。因此参考阿西莫夫的方法，提出了预测模型所必须满足的要求：
- Prediction’s validation. For every prediction of a model, one should be able to verify how strong the evidence is that supports the prediction.
- Prediction’s justification. For every prediction of a model, one should be able to understand which variables affect the prediction and to what extent.
- Prediction’s speculation. For every prediction of a model, one should be able to understand how the prediction would change if the values of the variables included in the model changed.
为了满足这些条件有两种方法：设计可解释模型（线性模型/基于规则的模型/带有少量参数的分类树），或者通过近似或简化设计模型无关的模型解释工具。

## Instance Level

书中将模型解释方法分为了实例级别与数据集级别分别进行讨论。实例级探索方法主要针对特定的单个观测值的预测结果进行解释，例如：
- 评估解释变量对于某个个体预测结果的影响
- 了解某些解释变量的值发生变化将如何影响模型的预测结果
- 分析模型不正确预测结果的原因
同时，作者将实例级探索方法分为了三类：
- 可变归因（variable attributions）：分析模型对特定实例的预测与平均预测有何不同，以及如何在解释变量之间分配这些差异。
- 使用模型的解释作为函数，并研究该函数在兴趣点（观察）周围的局部行为，对于不可知模型，需要通过简单的可知模型对其进行拟合。
- 研究如果单个解释变量的值发生变化，模型的预测将如何变化，即构建表示由单个解释变量的更改引起的模型预测变化曲线（CP profile）

![](../public\阅读笔记18\01.jpg)

### Break-down Plot for Additive Attributions

在可加性归因解释方法中，使用Break-down图来将模型预测分解为可归因与不同解释变量的贡献。

文中通过预测泰坦尼克号上一名乘客的生存概率进行举例说明：第一行显示了所有数据的模型预测的分布和平均值（红色点），当确定后续解释变量的值时，下一行显示预测的分布和平均值，最后一行显示了对特定兴趣实例的预测，C中的绿色和红色条分别表示平均预测的正和负变化（归因于解释变量的贡献）。

![](../public\阅读笔记18\02.jpg)

而图中所展示的模型解释方法只是对于解释变量加入顺序中一种的分析，更改解释变量的加入顺序，BD图给出的解释将发生变化：

![](../public\阅读笔记18\03.jpg)

BD方法所使用的实现更完备的可加性归因解释的方法是将解释变量作为随机变量，求解变更解释变量值过程中归因于解释变量的贡献（预测值变化）的期望。该方法具有简洁易懂、计算量小的有点，但是必须建立在解释变量间相互独立的基础上，并且解释变量的引入顺序会很大程度上影响解释结果（一种解决方法是每次分别计算重要性指标，并将重要性最高的变量放在开头）。

### Shapley Additive Explanations (SHAP) for Average Attributions

解决解释变量的排序问题的一种更好的方法是计算不同排序方式下，解释变量贡献的平均值，这一方法由合作博弈论中提出的Shapley值得出。对于shapley值的有效估计算法SHAP已经被广泛应用于各类模型解释之中。

使用25种不同的随机排序解释对上文中出现的例子就可以得到如下的结果，SHAP便是基于这一原理之上的高效估计算法：

![](../public\阅读笔记18\04.jpg)

Shapley值提供了一种统一的方法，可以将模型的预测分解为贡献，这些贡献可以累加地归因于不同的解释变量。shapley值的问题在于，其提供了解释变量针对于模型的累加贡献，如果模型不是可加的，则Shapley可能会产生误导（？）。

> An important drawback of Shapley values is that they provide additive contributions (attributions) of explanatory variables. If the model is not additive, then the Shapley values may be misleading. This issue can be seen as arising from the fact that, in cooperative games, the goal is to distribute the payoff among payers. However, in the predictive modelling context, we want to understand how do the players affect the payoff? Thus, we are not limited to independent payoff-splits for players.

### Local Interpretable Model-agnostic Explanations (LIME)

加性模型解释方法更适用于中低数量特征的模型解释，而对于含有大量解释变量的模型时，由于可加性模型通常会赋予变量非零的贡献度，因此效果不够理想。对于稀疏解释器最著名的方法是不可知模型局部解释（LIME），其核心思想在于通过已知的可解释模型对黑盒模型进行局部近似。图中解释了LIME方法的原理，两个颜色表示黑盒模型所形成的二分类器，而对于加号表示的解释目标点，首先人工引入数据点（目标点周围的点，点大小表示与目标点的距离），之后根据引入的数据点利用简单的白盒模型进行拟合（如线性回归模型）来近似黑盒模型在目标点附近的局部解释。

![](../public\阅读笔记18\05.jpg)

具体来讲，对于黑盒模型$f()$和兴趣点$\underline{x_*}$，LIME方法可以表示为：

$$\hat g = \arg \min_{g \in \mathcal{G}} L\{f, g, \nu(\underline{x}_*)\} + \Omega (g)$$

$\nu(\underline{x}_*)$表示兴趣点的相邻点，$L()$损失函数用于度量两个分布之间的差异，最后引入正则化修正来限制近似模型的复杂度。然而在这一过程中值得注意的一点是，原始模型与近似模型的输入变量所属的数据空间并不一致，原始解释变量可能包含大量的特征，而近似模型可以定义一个转换方式$h()$使输入维度远小于原始解释变量的维度。因此实现LIME的步骤主要包含三部分：

- 解释变量的表示：对于图像输入，可以采用超像素的方法（图像分割）；对于文本输入，可以使用词语集；对于连续值可以进行离散分割；对于类别值可以进行组合等。
- 围绕兴趣点的采样：对于二进制输入，可以进行简单的随机转换；对于连续输入则可以采用引入高斯噪声/离散化等方法。
- 拟合白盒模型：白盒模型一般选择简单的线性回归模型（LASSO等）。

对于前文的泰坦尼克号幸存预测例子，使用LIME方法，通过K-LASSO（K=3）进行解释，可以得到如下的结果：

![](../public\阅读笔记18\06.jpg)

LIME方法能够对无假设黑河模型进行解释，并且在局部保证高准确性，同时它能够将原始数据空间转换为更具解释性的低维空间，在图像/文本模型解释领域得到非常广泛的应用。但是对于其他类型的数据，如何进行解释变量的转换通常会影响解释的结果，这一问题尚未解决。而另一方面，使用白盒模型对黑盒模型进行拟合，近似质量也不总是在模型的每个位置都能够得到保证。

### Ceteris-paribus Profiles

前两种方法都是针对单个实例解释变量预测结果的解释，将预测分解为可归因于特定变量的成分。而Ceteris-paribus通过假设其他变量的值均不发生变化来了解目标变量值的变化如何影响模型的预测结果，即求解预测分布对于单个变量的条件期望。

本质上，CP图显示了因变量的条件期望（响应）对特定解释变量值的依赖性，同样以泰坦尼克号乘客生还预测为例，针对某一47岁第一阶层的乘客绘制cp图进行研究可以得到如下的结果，CP曲线所反映的结果更利于满足上文中提出规则的第三点，即推测解释变量发生变化所带来的预测值响应。

![](../public\阅读笔记18\07.jpg)

CP图方法会为每一个解释变量生成一个曲线，当变量维度很高时，很难在其中寻找出真正具有意义的变量，因此在CP图的基础上又提出了CP抖动方法用于衡量解释变量的重要性（类似于前两种方法所得到的变量重要性归因值）。实现方法是求解CP曲线的绝对值积分值，即CP图曲线下面积和：

$$vip_{CP}^j(\underline{x}_*) = \int_{\mathcal R} |h^{j}_{\underline{x}_*}(z) - f(\underline{x}_*)| g^j(z)dz=E_{X^j}\left\{|h^{j}_{\underline{x}_*}(X^j) - f(\underline{x}_*)|\right\}$$

![](../public\阅读笔记18\08.jpg)

CP图易于比较，因为可以通过覆盖两个或多个模型的CP图，以更好地了解模型之间的差异。还可以比较两个或多个实例，以更好地了解模型预测的稳定性，其也是进行敏感性分析的有用工具。但CP图最大的问题在于无法考虑变量之间的互相关作用，从而产生误导性结果，因为互相关的变量在根本上违反了一个发生变化而另一个不变的假设；另一方面，基于条形图的CP图针对一些数量较多的离散变量类型时难以处理。CP抖动方法也具有CP图所具有的局限性，同时它也不满足前两种方法具有的局部精确性和公理完整性（归因值只和等于预测值变化）。

### Local-diagnostics Plots

局部诊断图通过观察与兴趣点实例相似实例的表现来检查模型的局部表现，书中介绍了局部保真度和局部稳定性两种诊断策略。

局部保真度图用于围绕兴趣点的观察评估模型的局部预测性能。将数据集中与兴趣点相似的观察点记为兴趣点的邻近点，局部保真度的原理是比较兴趣点邻近点和整个测试集的残差（真实值与预测值之差）分布的差异。保真度对于检查感兴趣实例的模型拟合是否无偏很有用，因为在这种情况下，残差应该很小，并且它们的分布应该在0附近对称。

![](../public\阅读笔记18\09.jpg)

局部稳定性度量的是一些解释变量的微小变化是否会显著影响预测结果，即比较兴趣点邻近点之间模型解释结果的差异，图中以泰坦尼克号幸存预测为例画出了兴趣点周围10个邻近点的年龄变量CP剖面曲线，从图中可以断定模型的预测在兴趣点周围是稳定的。

![](../public\阅读笔记18\10.jpg)

### Summary

总的来说，对于书中的实例，首先使用不同的几种变量归因方法进行解释，结果一致表明，从预测约翰生存概率的角度来看，最重要的解释变量是年龄，性别，阶级和票价，但是，由于票价和舱位相关，并且年龄和性别的影响之间可能存在相互作用，因此加性分解提供的图像可能并不完全正确。接着，通过CP剖面图画出这四个特征的CP曲线可以进一步分析更改解释变量将对预测结果造成的影响。而第三行对于四个变量的分布特征进行了展示，可以帮助进一步了解兴趣点的特征与整体数据特征的关系。

![](../public\阅读笔记18\11.jpg)

不同模型解释技术通常取决于数据数据和研究的问题：
- 模型中的解释变量数量：变量数量很少时使用CP图最能反映每个变量的具体作用机理；变量数量较大时使用SHAP或BD方法可以快速获得变量的影响力；当变量数量非常大且不具有单独的实际意义时，使用LIME进行简化更为合理。
- 解释变量之间的相关性：相互作用归因/二维CP图等。
- 模型的交互作用：在具有交互作用的模型中，一个解释变量的作用可能取决于其他变量的值。
- 解释的稀疏性：预测模型可以使用数百个解释变量来生成特定实例的预测，但是其中可能只有少数变量具有实际的解释意义，使用LIME可以解决这一问题，但是需要合理的变量转换方法。

## Dataset Level

相比于基于个体实例的模型解释，数据集级别的模型解释可以提供以下信息：
- 模型中解释变量的重要性，发现重要解释变量或删除无关变量来简化模型
- 了解目标变量如何影响模型的整体预测（例如公寓位置如何导致价格的变化）
- 发现是否存在一些观测值导致模型产生错误的预测
- 研究模型的整体“性能”，例如，比较两个模型的预测平均准确性

### Model-performance Measures

书中首先介绍了用于模型整体性能评估的度量方法，这种评估分为两大类：对拟合优度的评估（GoF）和对预测准确性的评估（GoP）。GoF与以下问题有关：模型的预测在多大程度上解释（拟合）了用于开发模型的观测值的因变量值？另一方面，GoP与以下问题有关：该模型对新观测值的因变量值预测得如何？通常来说对于某些度量，它们对GoF或GoP的解释取决于它们是通过使用训练数据还是测试数据来计算的。文章根据因变量的性质分别介绍了各种模型评价度量指标。



