---
title: 'Explaining Explanations: Axiomatic Feature Interactions for  Deep Networks'
date: '2021-03-01 10:23:00'
tags: 
- 论文

category: Research
image: /阅读笔记17/01.jpg
---

[论文代码实现](https://github.com/suinleelab/path_explain)

## Introduction  and  Prior Work
特征归因方法是一种通过为每个输入特殊分配贡献度来衡量其对于输出值的贡献的模型解释方法，虽然这类方法可以理解每个特征对于预测结果的重要性，但是它无法解释特征重要的原因以及特征在模型中的相互作用。为了得到对于模型更全面的解释，既需要分析特征的归因，也需要分析特征的相互作用。

![](../public\阅读笔记17\01.jpg)

目前针对神经网络的特征相互作用解释方法：通过二阶导数特征解释贝叶斯神经网络（BNN）的全局相互作用；Neural Interaction Detection用于前馈神经网络的特征相互作用解释；Deep Feature Interaction Maps检测两个特征的相互作用；Contextual Decomposition用于解释前馈网络和卷积网络；Shapley Interaction Index从博弈论角度给出解释。这些方法或局限于网络结构，或计算困难，或不能满足特征归因中提出的公理要求。

文章主要贡献：
- 提出可以用于任何神经网络结构的特征对相互作用量化方法
- 提出特征相互作用解释方法应该满足的常识性公理
- 提出了针对基于ReLU的分段线性零二阶导数网络的理论计算方法

## Explaining  Explanations  with  Integrated  Hessians

分析问题可以发现，寻求模型解释中特征相互作用的解释实际上是寻求一种对于模型解释的解释，为了实现这一目标，文章从积分梯度特征归因方法出发，对于模型$f:R^d\Rightarrow R$，模型输入的第i个特征的积分梯度归因值表示为：

$$\phi_i^{IG}(f,x,x')=(x_i-x_i')\times\int^1_{\alpha=0}\frac{\partial f(x'+\alpha(x-x'))}{\partial x_i }d\alpha$$

该方法只要求模型$f$在从$x$到$x'$的路径上可微。而对于积分梯度计算过程本身，其也是一个可微的模型$\phi_i:R^d\Rightarrow R$。因此可以将积分梯度作用于其本身来解释特征$j$对于特征$i$的重要性的作用：

$$\Gamma_{i,j}(x)=\phi_j(\phi_i(x))$$

对于$i\not ={j}$，则可以推导出：

$$\Gamma_{i,j}(x)=(x_i -x_i')(x_j-x_j')\times\int^1_{\beta=0}\int^1_{\alpha=0}\alpha\beta\frac{\partial^2f(x'+\alpha\beta(x-x'))}{\partial x_i\partial x_j}d\alpha d\beta$$

在$i=j$的情况下，公式的计算具有一个额外的一阶项。文章将$\Gamma_{i,j}(x)$作为对于特征i的模型解释归因值，输入特征j的影响力。实际使用中基准值的选取标准复杂，文中使用训练集输入均值作为计算的基准值。

同样从积分梯度的公理性质出发，积分梯度满足完整性公理：$\Sigma_i\phi_i(x)=f(x)-f(x')$，推导可以得出上文的特征相互作用量化方法满足相互作用完整性公理：

$$\Sigma_i\Sigma_j\Gamma_{i,j}(x)=f(x)-f(x')$$

这一公理决定了每一对特征i,j的贡献之和解释了从$f(x')$到$f(x)$的模型输出，同时，满足这一公理将在模型输出和相互作用值之间建立量化联系，从而确定相互作用这一抽象概念的量化尺度。

另外，海森积分还满足自我完整性：$\Gamma_{i,i}(x)=\phi_i(x)-\Sigma_{j\not ={i}}\Gamma_{i,j}(x)$，即当特征i与所有其他特征均不存在相互作用，或只存在特征i一个输入特征时，特征i的海森积分值等于其梯度积分值$\Gamma_{i,i}(x)=\phi_i(x)$。

文章的appendix部分对于海森积分的其他公理性质给出了证明（敏感性、线性可加等）。特别的，海森梯度具有相互作用对称性，即$\Gamma_{i,j}(x)=\Gamma_{j,i}(x)$。这一结论可由主流神经网络及其激活函数的二阶导数连续性推导得知。

## Smoothing  ReLU Networks

海森积分的主要局限性在于在神经网络中被广泛使用的ReLU激活函数，基于ReLu激活函数的神经网络具有分段线性，且二阶导数为0，因此基于二阶方法的海森积分不能计算其特征相互作用值。

文中提出的解决方法是通过SoftPlus激活函数（$SoftPlus_{\beta}(x)=\frac{1}{\beta}log(1+e^{-\beta x})$）替代ReLU激活函数，一方面研究表明这种替代方法对模型输出和一阶归因模型输出带来的扰动最小；另一方面，通过这种方法平滑神经网络可以使得通过更少的离散步数达到对于海森积分的高精度估计。具体而言，文章中证明了以下的结论：

对于一个单层SoftPlus神经网络$f_{\beta}(x)=softplus_{\beta}(w^Tx)$和$d$维输入特征，到达估计容忍度$\epsilon$的插值点个数$k$满足：$k\leq O(\frac{d\beta^2}{\epsilon})$。

分析这一结论背后的原因可以发现，相比于原始网络，平滑后的网络其从基准值到输入值路径上的梯度方向更加趋向一致。

![](../public\阅读笔记17\02.jpg)

## Explanation of XOR function

为了解释为何特征相互作用比特诊归因值包含更丰富的信息，文章设计了XOR函数实验，该网络当两个输入相反时具有高值输出，在输入相同时具有零值输出，实验结果如图所示：

![](../public\阅读笔记17\03.jpg)

当使用零基准的积分梯度方法进行解释时，在两个特征输入均为1或0时得到了相同的归因值；而使用海森积分，当两个特征均为1时，产生不同的结果，该结果说明两个特征从0变为1都会带来正向作用，但这种正向作用被二者之间的相互作用带来的负面影响抵消（在两个特征同时为0时表现为全0值是由于基准值选择为0）。因此，可以表明特征相互作用相比于特征归因所具有的优势。

## Empirical Evaluation

文章对比了Shapley Interaction Index、Generalized Contextual Decomposition、Neural Interaction Detection以及Integrated Hessian等四种特征相互作用量化方法。

![](../public\阅读笔记17\04.jpg)

在计算时间方面（Shapley Interaction Index计算属于NP-hard问题，使用蒙特卡洛估计进行计算），分别测试输入特征数5，、50、500时，在1000个样本中各个方法的计算所有特征的相互作用值的花费时间。海森积分由于可以通过GPU并行计算，其计算时间几乎不受特征数量影响，而其他方法的计算复杂度至少为$O(d^2)$。

为了对比各个方法的解释效果，使用删除再训练评价基准（Remove and Retrain），即通过按照归因值顺序逐步消融高影响力特征并进行再训练计算其效果损失值，具有较快的效果下降的解释方法能够表明其能够更迅速的捕捉到最具影响力的特征。文章中设计了一个十个输入特征的回归任务，特征值由正态分布随机生成，标签则为20个特征相互作用之和累加得到，通过一个三层网络训练得到与正确结果0.99相关系数的预测模型作为实验对象。实验中使用了5种不同的的相互作用方法：tanhsum、cossum、multiply、max、min。tanhsum的结果在图中展示，结果表明海森积分具有最好的解释效果（与基于博弈论的Shapley Interaction Index效果相近）。

## Applications of Integrated Hessians

### NLP

自然语言处理领域在近年间经历了从卷积网络到循环网络再到预训练转换器结构的发展，虽然基于注意力机制的模型可以通过内部权重给出模型某些方面的解释，但一方面一些研究指出依赖于注意力权重进行解释并不可靠，另一方面注意力机制也无法涵盖词语与其上下文的相互作用。

通过在NLP任务中的实验，文章指出了两方面的问题。首先，通过海森积分，结果表明了转换器结构效果好于卷积网络等模型的原因：转换器结构模型能够从获得两个消极词汇例如not和bad之间相互关系的积极作用，而较为简单的模型则难以做到。除此之外，通过分析结果，文章还指出了NLP中的饱和现象：当多个消极词汇共同作用于一个名词时，这些词汇与名词间的相互关系表现出消极作用而这些形容词之间的相互作用表现出积极作用，也就是说，有越多的消极形容词用于形容同一名词，则单个小鸡形容词对于句子整体的分类起到的作用越小。

![](../public\阅读笔记17\06.jpg)

### Drug  combination  response prediction

![](../public\阅读笔记17\07.jpg)
