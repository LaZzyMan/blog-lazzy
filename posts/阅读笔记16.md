---
title: 'Understanding Global Feature Contributions With Additive Importance Measures'
date: '2021-03-02 09:23:00'
tags: 
- 论文

category: Research
image:
---

[论文代码实现](https://github.com/iancovert/sage/)

## Introduction

## Unifying  Global  Feature Importance Methods

文章引入了特征子集预测能力并介绍了如何通过加性重要性度量框架统一以往的方法。

### Predictive  Power of Feature  Subsets

特征重要性有许多不同的解释，文章中认为特征重要性应该与其为模型提供的预测能力相关。对于模型$f$和模型输入$X$，$X$由多个特征组成（$X_1,X_2,...,X_d$），对于特征子集$X_S={X_i|i\in S}, S\subseteq D,D= \{1,...,d\}$，将重要的特征定义为其缺失会显著降低$f$性能的特征，定义缺失特征$\overline{S}=D/S$的受限模型$f_S$为：

$$f_S(x_S)=E[f(X)|X_S=x_S]$$

从而使得缺失特征$X_{\overline{S}}$通过条件分布$p(X_{\overline{S}}|X_s=x_S)$进行边缘化，其中两个特殊情况$S=\phi,S=D$分别对应均值预测$f_{\phi}(x_{\phi})=E[f(X)]$和完整模型预测$f_D(x)=f(x)$。进一步的，给定损失函数$l$，$f_S$的总体风险定义为$E[l(f_S(X_S),Y)]$，其中期望针对的是数据分布$p(X,Y)$，将预测能力定义为在平均预测的基础上风险降低$v_f:P(D)\rightarrow R$：

$$v_f(S)=\underbrace{E[l(f_{\phi}(X_{\phi}),Y)]}_{Mean\,prediction}-\overbrace{E[l(f_S(X_S),Y)]}^{Using\,features\,X_S}$$

$v_f(S)$通过比较平均预测和$X_S$特征约束下预测损失的插值量化模型从特征集$X_S$中获得的预测能力，通常上这一数值将随着特征集$S$的扩大而增大。$v_f$是针对模型的预测能力指标，在此基础上也引入更为一般的预测能力指标，对于优化模型，定义从特征集$X_S$中获得的风险降低$v:P(D)\rightarrow R$为：

$$v(S)=\underbrace{min_{\hat{y}}E[l(\hat{y},Y)]}_{Optimal\,constant\,\hat{y}}-\overbrace{min_gE[l(g(X_S),Y)]}^{Optimal\,model\,using\,X_S}$$

左侧部分为最优常数预测损失而右侧为所有函数中最优优化模型$g$的损失，$v$代表了理论上可以从特征集$X_S$中获得的最大预测能力收益，通常情况下，模型$f$是对最优模型$g$的逼近，当$f$为最优模型时，$v$等同于$v_f$。

### Additive  Importance  Measures

在理想情况下，特征对于预测能力的贡献符合加性规则，即对于任何的子集$S,T,i\notin S,T$，始终有$v(S\cup{i})-v(S)=v(T\cup{i})-v(T)$。在这种情况下，可以直接将特征的预测能力贡献定义为其重要性$\phi_i=v({i})-v(\phi)$。然而实际情况下，特征贡献并不符合假性特征，而与特征加入的顺序直接相关。因此文章提出了一种可加性的重要性测度，并通过其度量与每个特征相关的性能提升：

可加重要性测度对于每个特征$i=1,...,d$分配重要性分数$\phi_i\in R$，并且其中存在一个常量$\phi_0$使得加性函数可以表示特征子集的越策能力$u(S)=\phi_0+\Sigma_{i\in S}\phi_i, u(S)\approx v(S)$。由于对于大部分预测问题，$v$都不具有可加性，因此$u$并不能完美的表示每个特征对于预测能力的贡献，因此对其进行较为景区的估计将有助于感知每个特征的重要性，下文中介绍了现有方法应对这一问题多进行的高精度估计策略。

### Existing Additive Importance  Measures

文章中指出通过加性重要性框架，可以统一众多文献中的特征重要性计算方法。这些方法可以分成三类，每一类中通过覆盖$P(D)$域的不同部分来进行加性模型$u$对$v$的精确估计。

![](../public\阅读笔记16\04.jpg)

第一种方法针对的域为：$(\{D\}\cup\{D/\{i\}\})\subset P(D)$，即每次从特征集中除外一个特征，典型代表为特征消融类方法，$f_i$表示出去单个特征训练得到的模型，特征重要性表示为：

$$\phi_i=E[l(f_i(X_{D/\{i\}}), Y)]-E[l(f(X), Y)]$$

通常来说，$\phi_0=min_{\hat{y}}E[l(\hat{y}, Y)]-E[l(f(X), Y)]-\Sigma_{i\in D}\phi_i$从而使得$u(D),u(D/\{i\}$估计值与$v$真实值相等。

第二种方法针对的域为：$(\{\phi\}\cup\{\{i\}|i\in D\})\subset P(D)$，即每次只包含一个特征，通过$X_i$与$Y$之间的双变量关联来量化特征的单独预测能力：

$$\phi_i=E[l(\hat{y}, Y)]-E[l(g_i(X_i), Y)]$$

这种情况下$\phi_0=0$，使得$u(\phi)=v(\phi)=0$。

然而以上两种方法都没有老绿道特征之间的相互关系，第三类方法的对象与为特征域的所有子集，从而解释特征之间的复杂交互，这类方法自然也包括消融和包含两种主要类别，文章中提出的SAGE也属于这一类方法。

## Shapley  Additive Global  Importance

为了量化模型对于每个特征的依赖性，文章提出了基于博弈论Shapley值的SAGE（Shapley additive global importance），通过检查其性质解释为何其可以理解为可加重要性测度，并给出了基于采样的估计方法。

### Shapley  Values for  Credit  Allocation

Shapley值是唯一满足公平公理的分配方案，对于合作博弈$w:P(D)\Rightarrow R$，我们希望分配给每个参与者的分数$\phi_i(w)$满足：

- Efficiency：$\Sigma^d_{i=1}\phi_i(w)=w(D)-w(\phi)$
- Symmetry：如果对于任何子集$S$，$w(S\cup\{i\})=w(S\cup\{j\})$，则$\phi_i(w)=\phi_j(w)$
- Dummy：如果参与则贡献为0，即$w(S)=w(S\cup\{i\})$对任何$S$成立，则$\phi_i(w)=0$
- Monotonicity：如果参与者$w$始终比参与者$w'$的贡献度大，即对于任何S有$w(S\cup\{i\})-w(S)\leq w'(S\cup\{i\})-w'(S)$，则$\phi_i(w)\leq \phi_i(w')$
- Linearity：如果合作博弈$w(S)=\Sigma^n_{k=1}c_kw_k(S)$有多个博弈（$w_1,...,w_n$）构成，则有$\phi_i(S)=\Sigma^n_{k=1}c_k\phi_i(w_k)$

满足这些性质的Shapley值表示为：

$$\phi_i(w)=\frac{1}{d}\sum_{S\subseteq D/\{i\}}(\begin{matrix}
    d-1 \\ |S|
\end{matrix})^{-1}(w(S\cup\{i\})-w(S))$$

该公式表明，每个Shapley值$\phi_i(w)$都是增加$i$到子集$S\subseteq D/\{i\}$带来增量的加权平均值。对于基于模型的预测能力，得到其SAGE值$\phi_i(v_f)$，从Shapley值和$v_f$的定义可以推导出其具有如下性质：

- 由Efficiency推得：$\sum^d_{i=1}\phi_i(v_f)=v_f(D)$
- 有Symmetry推得：具有确定关系（相关性为1）的特征对$(X_i,X_j)$具有相同的重要性，即对于任何$(S,x)$有$f_{S\cup\{i\}}(x_{S\cup\{i\}})=f_{S\cup\{j\}}(x_{S\cup\{j\}})$，则有$v_f(S\cup\{i\})=v_f(S\cup\{f\})$
- 如果$X_i$与$f(X)$独立，则$\phi_i(v_f)=0$
- 对于模型$f,f'$的输出$Y,Y'$有$v_f(S\cup\{i\})-v_f(S)\leq v_{f'}(S\cup\{i\})-v_{f'}(S)$，则有$\phi_i(v_f)\leq \phi_i(v_{f'})$
- 由于线性，SAGE值可以看作应英语模型损失函数的每个实例的SHAP值的期望，即博弈$v_{f,x,y}(S)=l(f_{\phi}(x_{\phi}),y)-l(f_S(x_S), y)$的Shapley值$\phi_i(v_{f,x,y})$。或者说SAGE值$\phi_i(v_f)=E_{XY}[\phi_i(v_{f},X,Y)]$

当损失函数使用交叉熵或MSE时且模型为最优时，SAGE值有一个更好的解释，例如在使用交叉熵损失函数时，重要性表现为将特征纳入不同特征集带来的输出Y的不确定性降低值（条件互信息）的加权平均。

### SAGE as an  Additive  Importance Measure

### Practical  SAGE  Approximation

## Related Work

## Experiments