---
title: 'A Unified Approach to Interpreting Model Predictions'
date: '2020-12-04 16:01:00'
tags: 
- 论文

category: Research
---

部分内容参考Christoph Molnar发布的书籍《Interpretable Machine Learning》

## Introduction

提出模型预测解释的统一方法：
- 将解释模型预测结果的过程本身定义为“解释模型”，进而定义加性特征归因方法(additive feature attribution methods)。
- 通过博弈论得到具有唯一解的加性特征归因方法，提出SHAP作为各类模型的统计近似度量。
- 提出新的SHAP估计方法并证明其更符合人类直觉。

## Additive Feature Attribution Methods

对于模型的解释最优的方法是使用原模型进行解释，而对于复杂的模型，例如深层网络，原模型不能用于进行解释，因而只能使用简化的近似模型。

局部方法：对于模型$f$和解释模型$g$，$f(x)$是基于输入$x$的预测结果，解释模型通常使用映射来简化输入$x=h_x(x')$，并且尽可能保证$g(z')\approx f(h_x(z'))$。

定义加性特征归因方法解释模型为一个二值变量$z_i\in\{0, 1\}^M$线性函数：

$$g(z')=\phi_0+\Sigma_{i=1}^M\phi_iz'_i$$

其中，$\phi_i\in R$且M表示简化后的输入特征数。解释模型通过$\phi_i$对每个特征进行归因，并将特征影响力之和逼近模型的原始输出。

### LIME
LIME(Local Interpretable Model-Agnostic Explanations)通过局部估计的方法解释模型的单个预测结果，使用上文中的加性特征归因模型，$x=h(x')$将用于解释的二值向量映射到原输入空间，并通过优化寻找$\phi$的解:

$$\xi=\mathop{\arg\min}_{g\in G}L(f,g,\pi_{x'})+\Omega(g).$$

在局部核$\pi_{x'}$加权下计算f和g的损失函数L（平方损失函数），并通过\Omega惩罚g的复杂度，整体可通过带有罚值的线性回归完成。

### DeepLIFT
DeepLIFT是深度学习的递归预测解释方法。对于每一个输入特征$x_i$使用$C_{\Delta x_iy_i}$表示该特征被赋予非原始值时的归因影响力。在DeepLIFT中，$x=h_x(x')$用于将二值化特征映射到原始特征空间，1代表输入使用原始值，0表示输入使用参考值（人为设置的特征非典型背景值）。

$$\sum_{i=1}^nC_{\Delta x_i\Delta o}=\Delta o$$

其中$o=f(x)$是模型的输出，$\Delta o=f(x)-f(r),\Delta x_i=x_i-r_i$，r代表输入参考值，统一到加性特征归因方法中，$\phi_i=C_{\Delta x_i\Delta o},\phi_0=f(r)$。

### Layer-Wise Relevance Propagation

逐层相关性传播方法解释模型预测相当于DeepLIFT算法的特殊情形，将所有神经元的参考值设置为0，即$x=h_x(x')$将二值化特征值转换到原始空间，1表示原始输入值，0表示0输入值。

### Classic Shapley Value Estimation

三种使用合作博弈论经典方程计算模型预测解释：
- Shapley regression values
- Shapley sampling values
- Quantitative Input Influence

Shapley regression values用于表示存在多重共线性的线性模型的特征重要性。它通过计算所有特征子集包含和不包含目标特征的预测结果差异加权求和作为特征的重要性：

$$\phi_i=\sum_{S\subseteq F /\{i\}}\frac{|S|!(|F|-|S|-1)!}{|F|!}[f_{S\cup\{i\}}(x_{S\cup\{i\}})-f_S(x_S)]$$

$h_x$通用将二值空间映射到原始空间，1表示输入值中包含该特征，0表示输入值中不包含目标特征，令$\phi_0=f_{ \emptyset}(\emptyset)$，则该方法也适用于加性特征归因方法。

Shapley sampling values方法在上一方法的基础上改进，通过采样估计归因值并通过积分方法近似从训练集中移除某一特征的结果从而降低计算量。因此也是一种加性特征归因方法。

## Simple Properties Uniquely Determine Additive Feature Attributions

加性特征归因方法的优点在于唯一解的存在和三个理想属性，这些属性与传统Shapley值估计方法类似。

### Property 1(Local accuracy)

$$f(x)=g(x')=\phi_0+\sum^M_{i=1}\phi_ix'_i$$

当$x=h_x(x')$时，解释模型输出与原模型输出一致。

### Property 2(Missingness)

$$x_i=0\Rightarrow\phi_i=0$$

缺失的特征没有归因影响力（特征观察不到，即特征在所有样本中特征值相同）。

### Property3(Consistency)

令$f_x(z')=f(h_x(z'))$，$z'/i\Rightarrow z_i=0$，则对于两个模型$f$和$f'$：

$$f'_x(z')-f'_X(z'/i)\geq f_x(z')-f_x(z'/i)$$

$$\phi_i(f',x)\geq \phi_i(f, x)$$

如果模型发生变化使得某一特征的贡献度增加，则该特征在解释模型中的归因影响力不会减少。

有且仅有一个加性特征归因解释模型$g$能够满足上述三个属性：

$$\phi_i(f,x)=\sum_{z'\subseteq x'} \frac{|z'|!(M-|z'|-1)!}{M!}[f_x(z')-f_x(z'/i)]$$

其中$|z'|$表示非0特征个数，$z'$中的非零项也是$x'$中非零项的子集。公式的含义在于求得特征i归因影响力在不同的特征排列顺序下的加权平均值，$M!$表示所有特征的排列数，$|z'|!$表示特征i之前的特征集的排列数，$(M-|z'|-1)!$表示特征i之后的特征集的排列数，特征i前后的特征集的加入顺序并不会影响$f_x(z')-f_x(z'/i$的值。

PS：Shapley Value最初出现在经济学领域，旨在考虑各个代理的贡献来公平的分配合作收益，其方法是计算边际贡献平均值，是满足效率性、对称性、虚拟性和可加性的唯一的归因方法（Efficiency, Symmetry, Dummy and Additivity）。

这一结论来自组合合作博弈论，$\phi_i$被称为Shapley值。因此对于给定的特征简化映射$h_x$，基于非Shapley理论的方法会违背局部精确性和一致性原则。Shapley值的实质意义是给定当前的一组特征值，其预测结果与模型使用**平均特征值**预测得到结果的差值。因此Shapley值的解释建立在使用所有特征的基础上（不能解释稀疏缺失值），且无法做出特征预测（例如，如果某地点阔叶林比例上升10%，分类为绿地的概率将提高20%）。

## SHAP(SHapley Additive exPlanation) Values

SHAP是原始模型的条件期望函数的Shapley值，$f_x(z')=f(h_x(z'))=E[f(z)|z_S]$，其中S是$z'$中非零索引的集合。$E[f(z)]$代表所有特征未知时，模型输出的期望，随着每个特征的加入直至$z_S$，模型输出最终到达$f(x)$，图中只表示了一种特征加入顺序，当特征之间存在非线性关系或者非独立时，特征加入顺序会影响计算结果，SHAP值是所有可能顺序得到的$\phi_i$值的均值。

![](../public\阅读笔记12\01.jpg)

SHAP中隐含$h_x(z')=z_S$，但由于大部分模型无法处理特征缺失值，因此通过$f(z_S)\approx E[f(z)|z_S]$进行估计。为了进一步简化运算，文中采用了两种黑盒模型估计方法：Shapley value sampling和Kernel SHAP；以及四种针对特定模型的估计方法。在进行这些近似时，特征独立性和模型非线性度是两种可选的近似假设：

![](../public\阅读笔记12\02.jpg)

### Model-Agnostic Approximations
在特征独立假设下，shapley sample values等方法通过采样方法单独计算单个特征的置换方法Shapley value。本文提出的Kernel SHAP方法用于针对小规模输入高效计算Shapley value。

**Kernel SHAP(Linear LIME + Shapley values)**

Linear LIME通过线性模型拟合原模型进行解释，其符合加性归因方法，但是由于其中损失函数$L$，权重核$\pi_{x'}$以及正则化函数$\Omega$通过启发式方法设定，其结果并不能达到Shapley value。Kernel SHAP目标则是用过非启发式方法设定这些参数来使其达到Shapley value，从而达到对于任意黑盒模型计算Shapley解释的目标。

$$\Omega(g)=0,$$

$$\pi_{x'}(z')=\frac{(M-1)}{(M choose|z'|)|z'|(M-|z'|)},$$

$$L(f,g,\pi_{x'})=\sum_{z'\in Z}[f(h^{-1}_x(z'))-g(z')]^2\pi_{x'}(z'),$$

其中$|z'|$表示$z'$中非零元素个数。

![](../public\阅读笔记12\03.jpg)

### Model-Specific Approximations

虽然kernel SHAP可以适用于任何模型，但是对于特定的模型，可以进一步提高计算效率。

**Linear SHAP**
对于特征相互独立的线性模型，SHAP值可以通过模型的权重系数进行近似：

$$f(x)=\sum_{j=1}^Mw_jx_j+b$$

$$\phi_0(f,x)=b,\phi_i(f,x)=w_j(x_j-E[x_j])$$

特征的归隐影响力可以直接通过权重系数和特征的偏差值计算。

**Deep SHAP (DeepLIFT + Shapley values)**

与Kernel SHAP相似，Deep SHAP在DeepLIFT满足加性归因方法的基础上，通过递归计算模型乘参数的方法使其得到Shapley value的近似估计：

![](../public\阅读笔记12\04.jpg)

## Summary
对于图网络的特征归因：

针对一整类特征，分析各类特征的归因影响力可以参考Shapley中组合博弈论的思想。

对图网络模型所有特征进行解释难以应用Shapley value。
图网络边特征与节点特征不能一一对应（由于全局特征也对节点具有影响，所以不能将邻接矩阵作为特征），一方面难以通过小规模数据计算期望带来巨大计算量，另一方面这种多对多关系也意味着所有节点共享着所有的边特征，特征期望相同在SHAP框架下不具有归因影响力。
