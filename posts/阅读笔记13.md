---
title: 'Axiomatic Attribution for Deep Networks'
date: '2020-12-07 17:23:00'
tags: 
- 论文

category: Research
---

## Motivation and Summary of Results

对于深层网络模型$F:R^n\rightarrow[0,1]$和输入特征$x=(x_1,...,x_n)\in R_n$，定义输入$x$对于基准输入$x'$的归因为$A_F(x,x')=(a_1,...,a_n)\in R_n$其中$a_i$是$x_i$对于预测结果$F(x)$的贡献度。

## Two Fundamental Axioms

### Axiom: Sensitivity(a)

对于任意的输入和基准，某一个特征的变化使得模型得到不同的预测结果，那么该特征应该拥有非零的归因值。

梯度归因违反了敏感性(a)：传统使用梯度作为显著性表示的解释方法具有梯度饱和的缺陷，即某个特征增强到一定程度后可能对于网络决策的贡献达到饱和从而重要性表现为0。

其他基于反向传播的归因方法：通过反向传播从预测结果逐层向输入特征传递，包括DeepLIFt、LRP、DeVonvNets等。DeConvNets违反了灵敏度原则；DeepLIFT和LRP方法通过设置基准的方法避免计算局部梯度而是计算大尺度的离散梯度来避免梯度饱和从而保证了灵敏性，然而其违反了实现不变性原则。

### Axiom: Implementation Invariance

对于两个网络，如果对于所有可能的输入，其输出均一致，那么这两个网络是功能等价的，虽然它们可能具有不同的实现方式。归因方法应该满足功能等价性，即归因方法应该将预测结果的成因分配给输入特征，而在这一过程中不依赖于模型的具体实现细节。因此DeepLIFT和LRP方法违背了这一原则。

具体来讲，假设$f$和$g$为系统的输入和输出，而$h$是模型实现的具体细节，则输入特征的梯度可以通过$\frac{\partial f}{\partial h}$来直接计算，或者通过链式法则计算$\frac{\partial f}{\partial g}=\frac{\partial f}{\partial g}\cdot \frac{\partial g}{\partial g}$，因此梯度方法可以忽略模型细节，达到实现不变性。而DeepLIFT和LRP通过计算离散梯度来满足敏感性原则，但离散梯度的计算过程并不试用链式法则，因此其计算过程与模型实现相关：
$$\frac{f(x_1)-f(x_0)}{g(x_1)-g(x_0)}\not ={} \frac{f(x_1)-f(x_0)}{h(x_1)-h(x_0)}\cdot \frac{h(x_1)-h(x_0)}{f(x_1)-f(x_0)}$$
不能满足实现不变性的归因方法可能会在归因过程中产生误解，例如如果网络的自由度大于一个功能所需的自由度，那么过多的网络参数可能会导致两组不同的值输出相同的功能，训练过程可能使他们收敛于任意一组值，而实际上他们所对应的功能是相同的。

## Our Method: Integrated Gradients

考虑基准值$x'$到输入值$x$之间的路径，积分梯度通过累计其间每一个点的梯度得到，即梯度函数在$x'$到$x$路径上的积分值：

$$IntegratedGrads_i(x)=(x_i-x_i')\times \int_{\alpha=0}^1\frac{\partial F(x'+\alpha\times(x-x'))}{\partial x_i}d_{\alpha}$$

积分梯度满足完备性原则（Completeness），即归因值之和等于F在$x$和$x'$处输出的差值。**在可以忽略基准值的情况下($F(x')\approx 0$)**，积分梯度也相当于将输出值分配给输入特征。

$$\sum_{i=1}^nIntegratedGrads_i(x)=F(x)-F'(x)$$

完备性可以推出敏感性(a)，或者作为对于敏感性(a)的强化；同时积分梯度也符合实现不变性，因为它使用网络表示的梯度进行计算而不是离散梯度。

## Uniqueness of Integrated Gradients

首先通过路径方法泛化积分梯度并证明积分梯度是满足理想原则的唯一方法，之后分析其原因。

### Path Methods

积分梯度实质上是对于基准值到输入值上直线路径积分的聚合，而实际上两点之间还存在着其他的非直线路径。

![](../public\阅读笔记13\01.jpg)

假设$\gamma=(\gamma_1,...,\gamma_n):[0,1]\Rightarrow R^n$是一个从$x$到$x'$的光滑路径函数，$\gamma(0)=x',\gamma(1)=x$，那么路径积分梯度就可以通过对$\gamma(\alpha)$在$\alpha\in[0,1]$上积分得到：

$$PathIntegratedGrads^r_i(x)=\int^1_{\alpha=0}\frac{\partial F(\gamma(\alpha))}{\partial\gamma_i(\alpha)}\frac{\partial\gamma_i(\alpha)}{\partial\alpha}d\alpha$$

其中$\frac{\partial F(x)}{\partial x_i}$表示$F$对于$x$在$i^th$的梯度。将基于这一理论的归因方法称为路径方法，可以证明，所有的路径方法均符合敏感性(a)、完整性和实现不变性，而积分梯度是路径方法中使用直线路径$\gamma=x'+\alpha\times(x-x')$的特殊情况。

**Axiom: Sensitivity(b)(Dummy).** 如果网络预测结果不依赖于某些变量，那么他们的归因值应该为0。这一原则是对敏感性(a)的补足。

**Axiom: Linearity.** 假设有两个模型$f_1,f_2$，目标模型是这两个模型的线性组合$a\times f_1+b\times f_2$，那么线性要求目标模型的归因应该也是这两个模型归因的线性组合$a\times A_1+b\times A_2$。

可以证明，路径归因方法是唯一的同时满足实现不变性、敏感性、线性和完整性的归因方法。

### Integrated Gradients is Symmetry-Preserving

**Symmetry-Preserving.** 交换两个输入值并不改变函数的输出，即$F(x,y)=F(y,x)$，那么对于具有相同的输入值和基准值的对称变量，具有对称保护性的归因方法应该给出相同的归因值，也就是说如果两个变量在模型中扮演的角色相同则它们应该拥有相同的归因值。

例如对于逻辑模型$\sigma(x_1+x_2+...+)$中，$x_1,x_2$是对称变量，那么对于$x_1=x_2=1,x_1'=x_2'=0$具有对称保护性的归因方法应该给出相同的归因值。

可以证明，积分梯度是具有对称保护性的唯一的路径归因方法。但如果允许多条路径组合进行归因，那么将存在其他方法满足这一原则，例如Shapley value方法，通过$n!$条路径求取平均值，每条路径代表特征的不同输入顺序。但这种方法得到而归因结果有时与积分梯度存在区别，例如$min(x_1,x_2)$，如果$x_1'=x_2'=0,x_1=1,x_2=3$那么积分梯度将会将结果的改变全部归因到$x_2$，而Shapley value方法则会将其平均归因到二者。

## Applying Integrated Gradients

### Selecting a Benchmark

基准线的选择大部分情况下接近于0，但并不是绝对，原则在于选择对于目标任务不包含任何信息的特征作为基准线。例如在图像识别算法中，黑色图像表示没有物体，而在这个意义上黑色图像并不是唯一的基准选择，由噪声构成的图像也有同样的特征，然而选择黑色图像的原因是其可能会使边缘特征更为明显；文本网络中输入全零向量作为基准也能取得很好的效果。但是这两者之间存在区别，黑色影响对于模型来说是有效输入，而全零向量对于文本网络并非有效输入而只是数学上仍然可以计算。

### Computing Integrated Gradients

积分梯度的计算可以通过黎曼积分有效逼近：

$$IntegratedGrads_i^{approx}(x)=(x_i-x_i')\times \sum^m_{k=1}\frac{\partial F(x'+\frac{k}{m}\times (x-x'))}{\partial x_i}\times \frac{1}{m}$$

其中m表示黎曼积分的步数，通常情况下20-300的步数足以近似这一积分（实际使用中20steps误差<10%，50steps误差<5%，100steps误差<1%）。

## Summary

（1）积分梯度能够达到Shapley Value相同的公理化归因结果，但是计算复杂度显著低于计算Shapley Value。

（2）基准值的选取对于积分梯度归因十分重要。